import asyncio
import random
import logging
import os
from ..actions.utils import stream_output
from ..actions.query_processing import plan_research_outline, get_search_results
from ..document import DocumentLoader, OnlineDocumentLoader, LangChainDocumentLoader
from ..utils.enum import ReportSource, ReportType
from ..utils.logging_config import get_json_handler
from ..actions.agent_creator import choose_agent


class ResearchConductor:
    """Manages and coordinates the research process."""

    def __init__(self, researcher):
        self.researcher = researcher
        self.logger = logging.getLogger('research')
        self.json_handler = get_json_handler()
        # Add cache for MCP results to avoid redundant calls
        self._mcp_results_cache = None
        # Track MCP query count for balanced mode
        self._mcp_query_count = 0

    async def plan_research(self, query, query_domains=None):
        """Gets the sub-queries from the query
        Args:
            query: original query
        Returns:
            List of queries
        """
        await stream_output(
            "logs",
            "planning_research",
            f"ğŸŒ æ­£åœ¨æµè§ˆç½‘ç»œä»¥äº†è§£æ›´å¤šå…³äºä»»åŠ¡çš„ä¿¡æ¯: {query}...",
            self.researcher.websocket,
        )

        search_results = await get_search_results(query, self.researcher.retrievers[0], query_domains, researcher=self.researcher)
        self.logger.info(f"å·²è·å–åˆå§‹æœç´¢ç»“æœ: {len(search_results)} æ¡")

        await stream_output(
            "logs",
            "planning_research",
            f"ğŸ¤” æ­£åœ¨è§„åˆ’ç ”ç©¶ç­–ç•¥å’Œå­ä»»åŠ¡...",
            self.researcher.websocket,
        )

        retriever_names = [r.__name__ for r in self.researcher.retrievers]
        # Remove duplicate logging - this will be logged once in conduct_research instead

        outline = await plan_research_outline(
            query=query,
            search_results=search_results,
            agent_role_prompt=self.researcher.role,
            cfg=self.researcher.cfg,
            parent_query=self.researcher.parent_query,
            report_type=self.researcher.report_type,
            cost_callback=self.researcher.add_costs,
            retriever_names=retriever_names,  # Pass retriever names for MCP optimization
            **self.researcher.kwargs
        )
        self.logger.info(f"å·²ç”Ÿæˆç ”ç©¶å¤§çº²: {outline}")
        return outline

    async def conduct_research(self):
        """Runs the GPT Researcher to conduct research"""
        if self.json_handler:
            self.json_handler.update_content("query", self.researcher.query)
        
        self.logger.info(f"å¼€å§‹ç ”ç©¶æŸ¥è¯¢: {self.researcher.query}")
        
        # Log active retrievers once at the start of research
        retriever_names = [r.__name__ for r in self.researcher.retrievers]
        self.logger.info(f"å¯ç”¨çš„æ£€ç´¢å™¨: {retriever_names}")
        
        # Reset visited_urls and source_urls at the start of each research task
        self.researcher.visited_urls.clear()
        research_data = []

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "starting_research",
                f"ğŸ” æ­£åœ¨å¼€å§‹ç ”ç©¶ä»»åŠ¡ '{self.researcher.query}'...",
                self.researcher.websocket,
            )
            await stream_output(
                "logs",
                "agent_generated",
                self.researcher.agent,
                self.researcher.websocket
            )

        # Choose agent and role if not already defined
        if not (self.researcher.agent and self.researcher.role):
            self.researcher.agent, self.researcher.role = await choose_agent(
                query=self.researcher.query,
                cfg=self.researcher.cfg,
                parent_query=self.researcher.parent_query,
                cost_callback=self.researcher.add_costs,
                headers=self.researcher.headers,
                prompt_family=self.researcher.prompt_family
            )
                
        # Check if MCP retrievers are configured
        has_mcp_retriever = any("mcpretriever" in r.__name__.lower() for r in self.researcher.retrievers)
        if has_mcp_retriever:
            self.logger.info("å·²é…ç½® MCP æ£€ç´¢å™¨ï¼Œå°†æŒ‰æ ‡å‡†ç ”ç©¶æµç¨‹ä½¿ç”¨")

        # Conduct research based on the source type
        if self.researcher.source_urls:
            self.logger.info("ä½¿ç”¨æä¾›çš„æ¥æº URL")
            research_data = await self._get_context_by_urls(self.researcher.source_urls)
            if research_data and len(research_data) == 0 and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "answering_from_memory",
                    f"ğŸ§ æ— æ³•åœ¨æä¾›çš„æ¥æºä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹...",
                    self.researcher.websocket,
                )
            if self.researcher.complement_source_urls:
                self.logger.info("è¡¥å……è¿›è¡Œç½‘ç»œæœç´¢")
                additional_research = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
                research_data += ' '.join(additional_research)
        elif self.researcher.report_source == ReportSource.Web.value:
            self.logger.info("ä½¿ç”¨æ‰€æœ‰å·²é…ç½®æ£€ç´¢å™¨è¿›è¡Œç½‘ç»œæœç´¢")
            research_data = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
        elif self.researcher.report_source == ReportSource.Local.value:
            self.logger.info("ä½¿ç”¨æœ¬åœ°æœç´¢")
            document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            self.logger.info(f"å·²åŠ è½½ {len(document_data)} ä¸ªæ–‡æ¡£")
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)

            research_data = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)
        # Hybrid search including both local documents and web sources
        elif self.researcher.report_source == ReportSource.Hybrid.value:
            if self.researcher.document_urls:
                document_data = await OnlineDocumentLoader(self.researcher.document_urls).load()
            else:
                document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)
            docs_context = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)
            web_context = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
            research_data = self.researcher.prompt_family.join_local_web_documents(docs_context, web_context)
        elif self.researcher.report_source == ReportSource.Azure.value:
            from ..document.azure_document_loader import AzureDocumentLoader
            azure_loader = AzureDocumentLoader(
                container_name=os.getenv("AZURE_CONTAINER_NAME"),
                connection_string=os.getenv("AZURE_CONNECTION_STRING")
            )
            azure_files = await azure_loader.load()
            document_data = await DocumentLoader(azure_files).load()  # Reuse existing loader
            research_data = await self._get_context_by_web_search(self.researcher.query, document_data)
            
        elif self.researcher.report_source == ReportSource.LangChainDocuments.value:
            langchain_documents_data = await LangChainDocumentLoader(
                self.researcher.documents
            ).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(langchain_documents_data)
            research_data = await self._get_context_by_web_search(
                self.researcher.query, langchain_documents_data, self.researcher.query_domains
            )
        elif self.researcher.report_source == ReportSource.LangChainVectorStore.value:
            research_data = await self._get_context_by_vectorstore(self.researcher.query, self.researcher.vector_store_filter)

        # Rank and curate the sources
        self.researcher.context = research_data
        if self.researcher.cfg.curate_sources:
            self.logger.info("æ­£åœ¨æ•´ç†æ¥æº")
            self.researcher.context = await self.researcher.source_curator.curate_sources(research_data)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "research_step_finalized",
                f"ç ”ç©¶æ­¥éª¤å·²å®Œæˆã€‚\nğŸ’¸ æ€»ç ”ç©¶æˆæœ¬: ${self.researcher.get_costs()}",
                self.researcher.websocket,
            )
            if self.json_handler:
                self.json_handler.update_content("costs", self.researcher.get_costs())
                self.json_handler.update_content("context", self.researcher.context)

        self.logger.info(f"ç ”ç©¶å®Œæˆã€‚ä¸Šä¸‹æ–‡å¤§å°: {len(str(self.researcher.context))}")
        return self.researcher.context

    async def _get_context_by_urls(self, urls):
        """Scrapes and compresses the context from the given urls"""
        self.logger.info(f"ä» URL è·å–ä¸Šä¸‹æ–‡: {urls}")
        
        new_search_urls = await self._get_new_urls(urls)
        self.logger.info(f"å¾…å¤„ç†çš„æ–° URL: {new_search_urls}")

        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)
        self.logger.info(f"å·²æŠ“å– {len(scraped_content)} ä¸ª URL çš„å†…å®¹")

        if self.researcher.vector_store:
            self.researcher.vector_store.load(scraped_content)

        context = await self.researcher.context_manager.get_similar_content_by_query(
            self.researcher.query, scraped_content
        )
        return context

    # Add logging to other methods similarly...

    async def _get_context_by_vectorstore(self, query, filter: dict | None = None):
        """
        Generates the context for the research task by searching the vectorstore
        Returns:
            context: List of context
        """
        self.logger.info(f"å¼€å§‹å‘é‡åº“æœç´¢æŸ¥è¯¢: {query}")
        context = []
        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query)
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸ æˆ‘å°†åŸºäºä»¥ä¸‹æŸ¥è¯¢è¿›è¡Œç ”ç©¶: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # Using asyncio.gather to process the sub_queries asynchronously
        context = await asyncio.gather(
            *[
                self._process_sub_query_with_vectorstore(sub_query, filter)
                for sub_query in sub_queries
            ]
        )
        return context

    async def _get_context_by_web_search(self, query, scraped_data: list | None = None, query_domains: list | None = None):
        """
        Generates the context for the research task by searching the query and scraping the results
        Returns:
            context: List of context
        """
        self.logger.info(f"å¼€å§‹ç½‘ç»œæœç´¢æŸ¥è¯¢: {query}")
        
        if scraped_data is None:
            scraped_data = []
        if query_domains is None:
            query_domains = []

        # **CONFIGURABLE MCP OPTIMIZATION: Control MCP strategy**
        mcp_retrievers = [r for r in self.researcher.retrievers if "mcpretriever" in r.__name__.lower()]
        
        # Get MCP strategy configuration
        mcp_strategy = self._get_mcp_strategy()
        
        if mcp_retrievers and self._mcp_results_cache is None:
            if mcp_strategy == "disabled":
                # MCP disabled - skip MCP research entirely
                self.logger.info("MCP å·²æŒ‰ç­–ç•¥ç¦ç”¨ï¼Œè·³è¿‡ MCP ç ”ç©¶")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_disabled",
                        f"âš¡ MCP ç ”ç©¶å·²è¢«é…ç½®ç¦ç”¨",
                        self.researcher.websocket,
                    )
            elif mcp_strategy == "fast":
                # Fast: Run MCP once with original query
                self.logger.info("MCP å¿«é€Ÿç­–ç•¥: ä»…ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿è¡Œä¸€æ¬¡")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_optimization",
                        f"ğŸš€ MCP å¿«é€Ÿæ¨¡å¼: ä»…å¯¹ä¸»æŸ¥è¯¢è¿è¡Œä¸€æ¬¡ (æ€§èƒ½æ¨¡å¼)",
                        self.researcher.websocket,
                    )
                
                # Execute MCP research once with the original query
                mcp_context = await self._execute_mcp_research_for_queries([query], mcp_retrievers)
                self._mcp_results_cache = mcp_context
                self.logger.info(f"MCP ç»“æœå·²ç¼“å­˜: å…± {len(mcp_context)} æ¡ä¸Šä¸‹æ–‡æ¡ç›®")
            elif mcp_strategy == "deep":
                # Deep: Will run MCP for all queries (original behavior) - defer to per-query execution
                self.logger.info("MCP æ·±åº¦ç­–ç•¥: å°†å¯¹æ‰€æœ‰æŸ¥è¯¢è¿è¡Œ")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_comprehensive",
                        f"ğŸ” MCP æ·±åº¦æ¨¡å¼: å°†å¯¹æ¯ä¸ªå­æŸ¥è¯¢è¿è¡Œï¼ˆå…¨é¢æ¨¡å¼ï¼‰",
                        self.researcher.websocket,
                    )
                # Don't cache - let each sub-query run MCP individually
            else:
                # Unknown strategy - default to fast
                self.logger.warning(f"æœªçŸ¥çš„ MCP ç­–ç•¥ '{mcp_strategy}'ï¼Œå°†å›é€€ä¸ºå¿«é€Ÿæ¨¡å¼")
                mcp_context = await self._execute_mcp_research_for_queries([query], mcp_retrievers)
                self._mcp_results_cache = mcp_context
                self.logger.info(f"MCP ç»“æœå·²ç¼“å­˜: å…± {len(mcp_context)} æ¡ä¸Šä¸‹æ–‡æ¡ç›®")

        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query, query_domains)
        self.logger.info(f"å·²ç”Ÿæˆå­æŸ¥è¯¢: {sub_queries}")
        
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸ æˆ‘å°†åŸºäºä»¥ä¸‹æŸ¥è¯¢è¿›è¡Œç ”ç©¶: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # Using asyncio.gather to process the sub_queries asynchronously
        try:
            context = await asyncio.gather(
                *[
                    self._process_sub_query(sub_query, scraped_data, query_domains)
                    for sub_query in sub_queries
                ]
            )
            self.logger.info(f"å·²æ±‡æ€» {len(context)} ä¸ªå­æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡")
            # Filter out empty results and join the context
            context = [c for c in context if c]
            if context:
                combined_context = " ".join(context)
                self.logger.info(f"åˆå¹¶åçš„ä¸Šä¸‹æ–‡å¤§å°: {len(combined_context)}")
                return combined_context
            return []
        except Exception as e:
            self.logger.error(f"ç½‘ç»œæœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
            return []

    def _get_mcp_strategy(self) -> str:
        """
        Get the MCP strategy configuration.
        
        Priority:
        1. Instance-level setting (self.researcher.mcp_strategy)
        2. Config file setting (self.researcher.cfg.mcp_strategy) 
        3. Default value ("fast")
        
        Returns:
            str: MCP strategy
                "disabled" = Skip MCP entirely
                "fast" = Run MCP once with original query (default)
                "deep" = Run MCP for all sub-queries
        """
        # Check instance-level setting first
        if hasattr(self.researcher, 'mcp_strategy') and self.researcher.mcp_strategy is not None:
            return self.researcher.mcp_strategy
        
        # Check config setting
        if hasattr(self.researcher.cfg, 'mcp_strategy'):
            return self.researcher.cfg.mcp_strategy
        
        # Default to fast mode
        return "fast"

    async def _execute_mcp_research_for_queries(self, queries: list, mcp_retrievers: list) -> list:
        """
        Execute MCP research for a list of queries.
        
        Args:
            queries: List of queries to research
            mcp_retrievers: List of MCP retriever classes
            
        Returns:
            list: Combined MCP context entries from all queries
        """
        all_mcp_context = []
        
        for i, query in enumerate(queries, 1):
            self.logger.info(f"æ‰§è¡Œ MCP ç ”ç©¶ æŸ¥è¯¢ {i}/{len(queries)}: {query}")
            
            for retriever in mcp_retrievers:
                try:
                    mcp_results = await self._execute_mcp_research(retriever, query)
                    if mcp_results:
                        for result in mcp_results:
                            content = result.get("body", "")
                            url = result.get("href", "")
                            title = result.get("title", "")
                            
                            if content:
                                context_entry = {
                                    "content": content,
                                    "url": url,
                                    "title": title,
                                    "query": query,
                                    "source_type": "mcp"
                                }
                                all_mcp_context.append(context_entry)
                        
                        self.logger.info(f"å·²æ·»åŠ  {len(mcp_results)} æ¡ MCP ç»“æœï¼ŒæŸ¥è¯¢: {query}")
                        
                        if self.researcher.verbose:
                            await stream_output(
                                "logs",
                                "mcp_results_cached",
                                f"âœ… å·²ç¼“å­˜æŸ¥è¯¢ {i}/{len(queries)} çš„ {len(mcp_results)} æ¡ MCP ç»“æœ",
                                self.researcher.websocket,
                            )
                except Exception as e:
                    self.logger.error(f"MCP ç ”ç©¶å‡ºé”™ï¼ŒæŸ¥è¯¢ '{query}': {e}")
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_cache_error",
                            f"âš ï¸ æŸ¥è¯¢ {i} çš„ MCP ç ”ç©¶å‡ºé”™ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–æ¥æº",
                            self.researcher.websocket,
                        )
        
        return all_mcp_context

    async def _process_sub_query(self, sub_query: str, scraped_data: list = [], query_domains: list = []):
        """Takes in a sub query and scrapes urls based on it and gathers context."""
        if self.json_handler:
            self.json_handler.log_event("sub_query", {
                "query": sub_query,
                "scraped_data_size": len(scraped_data)
            })
        
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_research",
                f"\nğŸ” æ­£åœ¨ä¸º '{sub_query}' è¿›è¡Œç ”ç©¶...",
                self.researcher.websocket,
            )

        try:
            # Identify MCP retrievers
            mcp_retrievers = [r for r in self.researcher.retrievers if "mcpretriever" in r.__name__.lower()]
            non_mcp_retrievers = [r for r in self.researcher.retrievers if "mcpretriever" not in r.__name__.lower()]
            
            # Initialize context components
            mcp_context = []
            web_context = ""
            
            # Get MCP strategy configuration
            mcp_strategy = self._get_mcp_strategy()
            
            # **CONFIGURABLE MCP PROCESSING**
            if mcp_retrievers:
                if mcp_strategy == "disabled":
                    # MCP disabled - skip entirely
                    self.logger.info(f"å­æŸ¥è¯¢å·²ç¦ç”¨ MCP: {sub_query}")
                elif mcp_strategy == "fast" and self._mcp_results_cache is not None:
                    # Fast: Use cached results
                    mcp_context = self._mcp_results_cache.copy()
                    
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_cache_reuse",
                            f"â™»ï¸ æ­£åœ¨ä¸º {sub_query} å¤ç”¨å·²ç¼“å­˜çš„ MCP ç»“æœï¼ˆ{len(mcp_context)} ä¸ªæ¥æºï¼‰",
                            self.researcher.websocket,
                        )
                    
                    self.logger.info(f"å­æŸ¥è¯¢å¤ç”¨ {len(mcp_context)} æ¡å·²ç¼“å­˜çš„ MCP ç»“æœ: {sub_query}")
                elif mcp_strategy == "deep":
                    # Deep: Run MCP for every sub-query
                    self.logger.info(f"æ­£åœ¨ä¸º {sub_query} è¿è¡Œæ·±åº¦ MCP ç ”ç©¶")
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_comprehensive_run",
                        f"ğŸ” æ­£åœ¨ä¸º {sub_query} è¿è¡Œæ·±åº¦ MCP ç ”ç©¶",
                            self.researcher.websocket,
                        )
                    
                    mcp_context = await self._execute_mcp_research_for_queries([sub_query], mcp_retrievers)
                else:
                    # Fallback: if no cache and not deep mode, run MCP for this query
                    self.logger.warning("MCP ç¼“å­˜ä¸å¯ç”¨ï¼Œå›é€€ä¸ºæŒ‰å­æŸ¥è¯¢æ‰§è¡Œ")
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_fallback",
                        f"ğŸ”Œ MCP ç¼“å­˜ä¸å¯ç”¨ï¼Œæ­£åœ¨ä¸º {sub_query} æ‰§è¡Œ MCP ç ”ç©¶",
                            self.researcher.websocket,
                        )
                    
                    mcp_context = await self._execute_mcp_research_for_queries([sub_query], mcp_retrievers)
            
            # Get web search context using non-MCP retrievers (if no scraped data provided)
            if not scraped_data:
                scraped_data = await self._scrape_data_by_urls(sub_query, query_domains)
                self.logger.info(f"æŠ“å–æ•°æ®é‡: {len(scraped_data)}")

            # Get similar content based on scraped data
            if scraped_data:
                web_context = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)
                self.logger.info(f"å­æŸ¥è¯¢ç½‘é¡µå†…å®¹é•¿åº¦: {len(str(web_context)) if web_context else 0} å­—ç¬¦")

            # Combine MCP context with web context intelligently
            combined_context = self._combine_mcp_and_web_context(mcp_context, web_context, sub_query)
            
            # Log context combination results
            if combined_context:
                context_length = len(str(combined_context))
                self.logger.info(f"å­æŸ¥è¯¢ '{sub_query}' åˆå¹¶ä¸Šä¸‹æ–‡: {context_length} å­—ç¬¦")
                
                if self.researcher.verbose:
                    mcp_count = len(mcp_context)
                    web_available = bool(web_context)
                    cache_used = self._mcp_results_cache is not None and mcp_retrievers and mcp_strategy != "deep"
                    cache_status = "ï¼ˆå·²ç¼“å­˜ï¼‰" if cache_used else ""
                    await stream_output(
                        "logs",
                        "context_combined",
                        f"ğŸ“š åˆå¹¶ç ”ç©¶ä¸Šä¸‹æ–‡: {mcp_count} ä¸ª MCP æ¥æº{cache_status}, {'åŒ…å«ç½‘é¡µå†…å®¹' if web_available else 'ä¸åŒ…å«ç½‘é¡µå†…å®¹'}",
                        self.researcher.websocket,
                    )
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°å­æŸ¥è¯¢çš„åˆå¹¶ä¸Šä¸‹æ–‡: {sub_query}")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "subquery_context_not_found",
                        f"ğŸ¤· æœªæ‰¾åˆ° '{sub_query}' çš„å†…å®¹...",
                        self.researcher.websocket,
                    )
            
            if combined_context and self.json_handler:
                self.json_handler.log_event("content_found", {
                    "sub_query": sub_query,
                    "content_size": len(str(combined_context)),
                    "mcp_sources": len(mcp_context),
                    "web_content": bool(web_context)
                })
                
            return combined_context
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å­æŸ¥è¯¢å‡ºé”™ {sub_query}: {e}", exc_info=True)
            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "subquery_error",
                    f"âŒ å¤„ç† '{sub_query}' å‡ºé”™: {str(e)}",
                    self.researcher.websocket,
                )
            return ""

    async def _execute_mcp_research(self, retriever, query):
        """
        Execute MCP research using the new two-stage approach.
        
        Args:
            retriever: The MCP retriever class
            query: The search query
            
        Returns:
            list: MCP research results
        """
        retriever_name = retriever.__name__
        
        self.logger.info(f"ä½¿ç”¨ {retriever_name} æ‰§è¡Œ MCP ç ”ç©¶ï¼ŒæŸ¥è¯¢: {query}")
        
        try:
            # Instantiate the MCP retriever with proper parameters
            # Pass the researcher instance (self.researcher) which contains both cfg and mcp_configs
            retriever_instance = retriever(
                query=query, 
                headers=self.researcher.headers,
                query_domains=self.researcher.query_domains,
                websocket=self.researcher.websocket,
                researcher=self.researcher  # Pass the entire researcher instance
            )
            
            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_retrieval_stage1",
                    f"ğŸ§  é˜¶æ®µ 1: ä¸º {query} é€‰æ‹©æœ€ä¼˜ MCP å·¥å…·",
                    self.researcher.websocket,
                )
            
            # Execute the two-stage MCP search
            results = retriever_instance.search(
                max_results=self.researcher.cfg.max_search_results_per_query
            )
            
            if results:
                result_count = len(results)
                self.logger.info(f"MCP ç ”ç©¶å®Œæˆ: æ¥è‡ª {retriever_name} çš„ {result_count} æ¡ç»“æœ")
                
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_research_complete",
                        f"ğŸ¯ MCP ç ”ç©¶å®Œæˆ: è·å¾— {result_count} æ¡æ™ºèƒ½ç»“æœ",
                        self.researcher.websocket,
                    )
                
                return results
            else:
                self.logger.info(f"{retriever_name} çš„ MCP ç ”ç©¶æœªè¿”å›ç»“æœ")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_no_results",
                        f"â„¹ï¸ MCP æœªæ‰¾åˆ°ä¸ {query} ç›¸å…³çš„ä¿¡æ¯",
                        self.researcher.websocket,
                    )
                return []
                
        except Exception as e:
            self.logger.error(f"{retriever_name} çš„ MCP ç ”ç©¶å‡ºé”™: {str(e)}")
            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_research_error",
                    f"âš ï¸ MCP ç ”ç©¶å‡ºé”™: {str(e)} - ç»§ç»­ä½¿ç”¨å…¶ä»–æ¥æº",
                    self.researcher.websocket,
                )
            return []

    def _combine_mcp_and_web_context(self, mcp_context: list, web_context: str, sub_query: str) -> str:
        """
        Intelligently combine MCP and web research context.
        
        Args:
            mcp_context: List of MCP context entries
            web_context: Web research context string  
            sub_query: The sub-query being processed
            
        Returns:
            str: Combined context string
        """
        combined_parts = []
        
        # Add web context first if available
        if web_context and web_context.strip():
            combined_parts.append(web_context.strip())
            self.logger.debug(f"å·²æ·»åŠ ç½‘é¡µä¸Šä¸‹æ–‡: {len(web_context)} å­—ç¬¦")
        
        # Add MCP context with proper formatting
        if mcp_context:
            mcp_formatted = []
            
            for i, item in enumerate(mcp_context):
                content = item.get("content", "")
                url = item.get("url", "")
                title = item.get("title", f"MCP ç»“æœ {i+1}")
                
                if content and content.strip():
                    # Create a well-formatted context entry
                    if url and url != f"mcp://llm_analysis":
                        citation = f"\n\n*æ¥æº: {title} ({url})*"
                    else:
                        citation = f"\n\n*æ¥æº: {title}*"
                    
                    formatted_content = f"{content.strip()}{citation}"
                    mcp_formatted.append(formatted_content)
            
            if mcp_formatted:
                # Join MCP results with clear separation
                mcp_section = "\n\n---\n\n".join(mcp_formatted)
                combined_parts.append(mcp_section)
                self.logger.debug(f"å·²æ·»åŠ  {len(mcp_context)} æ¡ MCP ä¸Šä¸‹æ–‡æ¡ç›®")
        
        # Combine all parts
        if combined_parts:
            final_context = "\n\n".join(combined_parts)
            self.logger.info(f"å­æŸ¥è¯¢ '{sub_query}' åˆå¹¶ä¸Šä¸‹æ–‡: å…± {len(final_context)} å­—ç¬¦")
            return final_context
        else:
            self.logger.warning(f"å­æŸ¥è¯¢æ— å¯åˆå¹¶ä¸Šä¸‹æ–‡: {sub_query}")
            return ""

    async def _process_sub_query_with_vectorstore(self, sub_query: str, filter: dict | None = None):
        """Takes in a sub query and gathers context from the user provided vector store

        Args:
            sub_query (str): The sub-query generated from the original query

        Returns:
            str: The context gathered from search
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_with_vectorstore_research",
                f"\nğŸ” æ­£åœ¨ä¸º '{sub_query}' è¿›è¡Œç ”ç©¶...",
                self.researcher.websocket,
            )

        context = await self.researcher.context_manager.get_similar_content_by_query_with_vectorstore(sub_query, filter)

        return context

    async def _get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """

        new_urls = []
        for url in url_set_input:
            if url not in self.researcher.visited_urls:
                self.researcher.visited_urls.add(url)
                new_urls.append(url)
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "added_source_url",
                        f"âœ… å·²å°†æ¥æº URL åŠ å…¥ç ”ç©¶: {url}\n",
                        self.researcher.websocket,
                        True,
                        url,
                    )

        return new_urls

    async def _search_relevant_source_urls(self, query, query_domains: list | None = None):
        new_search_urls = []
        if query_domains is None:
            query_domains = []

        # Iterate through the currently set retrievers
        # This allows the method to work when retrievers are temporarily modified
        for retriever_class in self.researcher.retrievers:
            # Skip MCP retrievers as they don't provide URLs for scraping
            if "mcpretriever" in retriever_class.__name__.lower():
                continue
                
            try:
                # Instantiate the retriever with the sub-query
                retriever = retriever_class(query, query_domains=query_domains)

                # Perform the search using the current retriever
                search_results = await asyncio.to_thread(
                    retriever.search, max_results=self.researcher.cfg.max_search_results_per_query
                )

                # Collect new URLs from search results
                search_urls = [url.get("href") for url in search_results if url.get("href")]
                new_search_urls.extend(search_urls)
            except Exception as e:
                self.logger.error(f"ä½¿ç”¨ {retriever_class.__name__} æœç´¢å‡ºé”™: {e}")

        # Get unique URLs
        new_search_urls = await self._get_new_urls(new_search_urls)
        random.shuffle(new_search_urls)

        return new_search_urls

    async def _scrape_data_by_urls(self, sub_query, query_domains: list | None = None):
        """
        Runs a sub-query across multiple retrievers and scrapes the resulting URLs.

        Args:
            sub_query (str): The sub-query to search for.

        Returns:
            list: A list of scraped content results.
        """
        if query_domains is None:
            query_domains = []

        new_search_urls = await self._search_relevant_source_urls(sub_query, query_domains)

        # Log the research process if verbose mode is on
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "researching",
                f"ğŸ¤” æ­£åœ¨è·¨å¤šä¸ªæ¥æºæ£€ç´¢ç›¸å…³ä¿¡æ¯...\n",
                self.researcher.websocket,
            )

        # Scrape the new URLs
        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)

        if self.researcher.vector_store:
            self.researcher.vector_store.load(scraped_content)

        return scraped_content

    async def _search(self, retriever, query):
        """
        Perform a search using the specified retriever.
        
        Args:
            retriever: The retriever class to use
            query: The search query
            
        Returns:
            list: Search results
        """
        retriever_name = retriever.__name__
        is_mcp_retriever = "mcpretriever" in retriever_name.lower()
        
        self.logger.info(f"ä½¿ç”¨ {retriever_name} æœç´¢ï¼ŒæŸ¥è¯¢: {query}")
        
        try:
            # Instantiate the retriever
            retriever_instance = retriever(
                query=query, 
                headers=self.researcher.headers,
                query_domains=self.researcher.query_domains,
                websocket=self.researcher.websocket if is_mcp_retriever else None,
                researcher=self.researcher if is_mcp_retriever else None
            )
            
            # Log MCP server configurations if using MCP retriever
            if is_mcp_retriever and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_retrieval",
                    f"ğŸ”Œ æ­£åœ¨æŸ¥è¯¢ MCP æœåŠ¡å™¨è·å–ä¿¡æ¯: {query}",
                    self.researcher.websocket,
                )
            
            # Perform the search
            if hasattr(retriever_instance, 'search'):
                results = retriever_instance.search(
                    max_results=self.researcher.cfg.max_search_results_per_query
                )
                
                # Log result information
                if results:
                    result_count = len(results)
                    self.logger.info(f"ä» {retriever_name} æ”¶åˆ° {result_count} æ¡ç»“æœ")
                    
                    # Special logging for MCP retriever
                    if is_mcp_retriever:
                        if self.researcher.verbose:
                            await stream_output(
                                "logs",
                                "mcp_results",
                                f"âœ“ å·²ä» MCP æœåŠ¡å™¨æ£€ç´¢åˆ° {result_count} æ¡ç»“æœ",
                                self.researcher.websocket,
                            )
                        
                        # Log result details
                        for i, result in enumerate(results[:3]):  # Log first 3 results
                            title = result.get("title", "æ— æ ‡é¢˜")
                            url = result.get("href", "æ—  URL")
                            content_length = len(result.get("body", "")) if result.get("body") else 0
                            self.logger.info(f"MCP ç»“æœ {i+1}: '{title}' æ¥è‡ª {url} ({content_length} å­—ç¬¦)")
                            
                        if result_count > 3:
                            self.logger.info(f"... ä»¥åŠå¦å¤– {result_count - 3} æ¡ MCP ç»“æœ")
                else:
                    self.logger.info(f"{retriever_name} æœªè¿”å›ç»“æœ")
                    if is_mcp_retriever and self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_no_results",
                            f"â„¹ï¸ MCP æœåŠ¡å™¨æœªæ‰¾åˆ°ä¸ {query} ç›¸å…³çš„ä¿¡æ¯",
                            self.researcher.websocket,
                        )
                
                return results
            else:
                self.logger.error(f"æ£€ç´¢å™¨ {retriever_name} æ²¡æœ‰ search æ–¹æ³•")
                return []
        except Exception as e:
            self.logger.error(f"ä½¿ç”¨ {retriever_name} æœç´¢å‡ºé”™: {str(e)}")
            if is_mcp_retriever and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_error",
                    f"âŒ ä» MCP æœåŠ¡å™¨æ£€ç´¢ä¿¡æ¯å‡ºé”™: {str(e)}",
                    self.researcher.websocket,
                )
            return []
            
    async def _extract_content(self, results):
        """
        Extract content from search results using the browser manager.
        
        Args:
            results: Search results
            
        Returns:
            list: Extracted content
        """
        self.logger.info(f"æ­£åœ¨ä» {len(results)} æ¡æœç´¢ç»“æœä¸­æå–å†…å®¹")
        
        # Get the URLs from the search results
        urls = []
        for result in results:
            if isinstance(result, dict) and "href" in result:
                urls.append(result["href"])
        
        # Skip if no URLs found
        if not urls:
            return []
            
        # Make sure we don't visit URLs we've already visited
        new_urls = [url for url in urls if url not in self.researcher.visited_urls]
        
        # Return empty if no new URLs
        if not new_urls:
            return []
            
        # Scrape the content from the URLs
        scraped_content = await self.researcher.scraper_manager.browse_urls(new_urls)
        
        # Add the URLs to visited_urls
        self.researcher.visited_urls.update(new_urls)
        
        return scraped_content
        
    async def _summarize_content(self, query, content):
        """
        Summarize the extracted content.
        
        Args:
            query: The search query
            content: The extracted content
            
        Returns:
            str: Summarized content
        """
        self.logger.info(f"æ­£åœ¨ä¸ºæŸ¥è¯¢ç”Ÿæˆæ‘˜è¦: {query}")
        
        # Skip if no content
        if not content:
            return ""
            
        # Summarize the content using the context manager
        summary = await self.researcher.context_manager.get_similar_content_by_query(
            query, content
        )
        
        return summary
        
    async def _update_search_progress(self, current, total):
        """
        Update the search progress.
        
        Args:
            current: Current number of sub-queries processed
            total: Total number of sub-queries
        """
        if self.researcher.verbose and self.researcher.websocket:
            progress = int((current / total) * 100)
            await stream_output(
                "logs",
                "research_progress",
                f"ğŸ“Š ç ”ç©¶è¿›åº¦: {progress}%",
                self.researcher.websocket,
                True,
                {
                    "current": current,
                    "total": total,
                    "progress": progress
                }
            )
